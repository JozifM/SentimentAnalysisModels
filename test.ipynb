{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import TFAutoModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin', engine='python', header=None)\n",
    "df.columns = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "df = df.sample(n=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695325     0\n",
      "223711     0\n",
      "491579     0\n",
      "438065     0\n",
      "1436152    1\n",
      "          ..\n",
      "1526877    1\n",
      "1226042    1\n",
      "214676     0\n",
      "1385448    1\n",
      "413250     0\n",
      "Name: target, Length: 2000, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3709/2086713888.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[labels == 4] = 1\n"
     ]
    }
   ],
   "source": [
    "X = df['text']\n",
    "labels = df['target']\n",
    "labels[labels == 4] = 1\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokenizer = Tokenizer()\\ntokenizer.fit_on_texts(X)\\nsequences = tokenizer.texts_to_sequences(X)'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize and pad the sequences\n",
    "'''tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the input data\n",
    "input_ids = []\n",
    "for example in X:\n",
    "    tokens = tokenizer.encode(example, add_special_tokens=True, max_length=50, padding='max_length', truncation=True)\n",
    "    input_ids.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1030, 2123, 2102, 4783, 19895, 23606, 5480, 1045, 2359, 2000, 2272, 2061, 2919, 999, 999, 2045, 2001, 2053, 2380, 3258, 2290, 2054, 2061, 2412, 2295, 999, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tf.keras.utils.to_categorical(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the attention mask matrix\n",
    "attention_mask = np.ones_like(input_ids)\n",
    "padded_positions = np.equal(input_ids, 0)\n",
    "attention_mask[padded_positions] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 50\n",
    "\n",
    "Xids = np.zeros((len(df), SEQ_LEN))\n",
    "Xmask = np.zeros((len(df), SEQ_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 50)\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(50,), dtype=float64, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>, 'attention_mask': <tf.Tensor: shape=(50,), dtype=float64, numpy=\n",
      "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])>}, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 0.], dtype=float32)>)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((Xids, Xmask, labels))\n",
    "\n",
    "def map_func(sequence, attention, labels):\n",
    "    return {'input_ids': sequence, 'attention_mask': attention}, labels\n",
    "\n",
    "dataset = dataset.map(map_func)\n",
    "\n",
    "for i in dataset.take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_LEN = len(df)\n",
    "\n",
    "train_len = 0.8  # 90-10 split\n",
    "val_len = 0.5\n",
    "\n",
    "train = dataset.take(round(DS_LEN*train_len))  # get first 90% of batches\n",
    "test = dataset.skip(round(DS_LEN*train_len))  # skip first 90% and keep final 10%\n",
    "\n",
    "test = dataset.take(round(DS_LEN*val_len))  # get first 90% of batches\n",
    "val = dataset.skip(round(DS_LEN*val_len))  # skip first 90% and keep final 10%\n",
    "\n",
    "\n",
    "\n",
    "del dataset  # optionally, delete dataset to free up disk-space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Define the BERT model\n",
    "bert = TFAutoModel.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tf.keras.layers.Input(shape=(50,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(50,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# we consume the last_hidden_state tensor from bert (discarding pooled_outputs)\n",
    "embeddings = bert(input_ids, attention_mask=mask)[0]\n",
    "\n",
    "X = tf.keras.layers.LSTM(64)(embeddings)\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dense(64, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.1)(X)\n",
    "y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)\n",
    "\n",
    "# define input and output layers of our model\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
    "\n",
    "# freeze the BERT layer - otherwise we will be training 100M+ parameters...\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()  # categorical = one-hot\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "63/63 [==============================] - 256s 4s/step - loss: 0.7201 - accuracy: 0.5070\n",
      "Epoch 2/40\n",
      "63/63 [==============================] - 245s 4s/step - loss: 0.6957 - accuracy: 0.4940\n",
      "Epoch 3/40\n",
      "63/63 [==============================] - 267s 4s/step - loss: 0.6941 - accuracy: 0.4950\n",
      "Epoch 4/40\n",
      "39/63 [=================>............] - ETA: 3:05 - loss: 0.6951 - accuracy: 0.4679"
     ]
    }
   ],
   "source": [
    "history = model.fit(train, validation_data=val, epochs=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentimentAnalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
